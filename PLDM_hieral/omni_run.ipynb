{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "SZmf53FjLP0d",
   "metadata": {
    "id": "SZmf53FjLP0d"
   },
   "source": [
    "# PLDM TwoRooms (L1 vs L2)\n",
    "\n",
    "Upload the tarball created by `PLDM_hieral/colab_pack.sh`, or clone the repo manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd226449",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "fd226449",
    "outputId": "5718a4cf-3f6d-4920-a45d-e814f65f801f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üì „Åì„Åì„Å´„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÂá¶ÁêÜ„ÅØËá™Âãï„ÅßÂßã„Åæ„Çä„Åæ„ÅôÔºâ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb824d37c37f4ffc91e51362dc08fa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.tar.gz,.tar', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2830d4278426470784d3b5008d3d61fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "\n",
    "OUTPUT_ROOT = './WM-JEPA' \n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# --- 1. „Ç¶„Ç£„Ç∏„Çß„ÉÉ„Éà„Å®Âá∫Âäõ„Ç®„É™„Ç¢„ÅÆ‰ΩúÊàê ---\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.tar.gz,.tar',\n",
    "    multiple=False\n",
    ")\n",
    "# „É≠„Ç∞„ÇíË°®Á§∫„Åô„Çã„Åü„ÇÅ„ÅÆÂ∞ÇÁî®„Ç®„É™„Ç¢\n",
    "log_output = widgets.Output()\n",
    "\n",
    "# --- 2. „Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂæå„Å´ÂÆüË°å„Åó„Åü„ÅÑÂá¶ÁêÜ„ÇíÈñ¢Êï∞„Å´„Åô„Çã ---\n",
    "def process_uploaded_file(change):\n",
    "    # change['new'] „Å´Êñ∞„Åó„ÅÑÂÄ§„ÅåÂÖ•„Å£„Å¶„ÅÑ„Åæ„Åô\n",
    "    if not change['new']:\n",
    "        return\n",
    "\n",
    "    with log_output:\n",
    "        clear_output() # Ââç„ÅÆ„É≠„Ç∞„Åå„ÅÇ„Çå„Å∞Ê∂à„Åô\n",
    "        print(\"üì• „Éï„Ç°„Ç§„É´„ÇíÊ§úÁü•„Åó„Åæ„Åó„Åü„ÄÇÂá¶ÁêÜ„ÇíÈñãÂßã„Åó„Åæ„Åô...\")\n",
    "        \n",
    "        # „Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Éï„Ç°„Ç§„É´„ÅÆÂèñÂæó (v8Á≥ªÂØæÂøú)\n",
    "        uploaded_file = change['new'][0]\n",
    "        filename = uploaded_file['name']\n",
    "        content = uploaded_file['content']\n",
    "        \n",
    "        # ‰øùÂ≠ò\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        print(f\"‚úÖ ‰øùÂ≠òÂÆå‰∫Ü: {filename}\")\n",
    "\n",
    "        # Ëß£ÂáçÂá¶ÁêÜ\n",
    "        if filename.endswith(('.tar.gz', '.tar', '.tgz')):\n",
    "            print(f\"üì¶ Ëß£Âáç‰∏≠: {filename} ...\")\n",
    "            os.system(f\"tar -xzf {filename} 2>&1 | grep -v 'Ignoring unknown extended header'\")\n",
    "            print(\"üéâ „Åô„Åπ„Å¶„ÅÆÂá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è tar„Éï„Ç°„Ç§„É´„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\")\n",
    "\n",
    "# --- 3. „Ç¶„Ç£„Ç∏„Çß„ÉÉ„Éà„Å´„ÄåÂ§âÂåñ„Åå„ÅÇ„Å£„Åü„ÇâÈñ¢Êï∞„ÇíÂÆüË°å„Åõ„Çà„Äç„Å®ÂëΩ‰ª§„Åô„Çã ---\n",
    "uploader.observe(process_uploaded_file, names='value')\n",
    "\n",
    "# --- 4. ÁîªÈù¢„Å´Ë°®Á§∫ ---\n",
    "print(\"‚Üì „Åì„Åì„Å´„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÂá¶ÁêÜ„ÅØËá™Âãï„ÅßÂßã„Åæ„Çä„Åæ„ÅôÔºâ\")\n",
    "display(uploader, log_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d7469a-8b55-4e0a-a808-bd114a663e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/self.model = torch.compile(self.model)/# self.model = torch.compile(self.model)/' */train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f8380c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7f8380c",
    "outputId": "c58383f3-92f1-4fde-877b-a791ce7ababc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==2.5.1 torchvision==0.20.1 gymnasium==1.0.0 gym==0.23.1 numpy==1.26.4 d4rl==1.1 mujoco==3.2.6 mujoco-py==2.1.2.14 wandb tqdm zarr arm-pytorch-utilities\n",
    "!pip install -q gdown omegaconf statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d3bf3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2d3bf3b",
    "outputId": "dbd0257c-a0e7-48e0-d501-bbb66b628294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ce6a45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0ce6a45",
    "outputId": "f63a07e0-8e07-42a7-c3d1-7e9551eb3a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1NwR-ui-akIgR2xcoJHYiHjOk9U5bYCa0\n",
      "From (redirected): https://drive.google.com/uc?id=1NwR-ui-akIgR2xcoJHYiHjOk9U5bYCa0&confirm=t&uuid=b93b3642-d67d-4f28-b376-88fe963b07db\n",
      "To: /workspace/WM-JEPA/PLDM_hieral/pldm_envs/wall/presaved_datasets/wall_data.tar.gz\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 535M/535M [00:16<00:00, 32.7MB/s]\n",
      "ds_size_1269_no_images.npz\n",
      "ds_size_1500K_no_images.npz\n",
      "ds_size_20312_no_images.npz\n",
      "ds_size_325K_no_images.npz\n",
      "ds_size_5078_no_images.npz\n",
      "ds_size_634_no_images.npz\n",
      "ds_size_81250_no_images.npz\n",
      "good_quality_data_no_images.npz\n",
      "len_17_no_images.npz\n",
      "len_33_no_images.npz\n",
      "len_65_no_images.npz\n",
      "noise_mix_0.001_no_images.npz\n",
      "noise_mix_0.01_no_images.npz\n",
      "noise_mix_0.02_no_images.npz\n",
      "noise_mix_0.04_no_images.npz\n",
      "noise_mix_0.08_no_images.npz\n",
      "noise_mix_0.16_no_images.npz\n",
      "random_trajectories_no_images.npz\n",
      "wc_rate_0.npz\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 25.56it/s]\n"
     ]
    }
   ],
   "source": [
    "!bash -c \"cd pldm_envs/wall && bash presaved_datasets/download_all.sh\"\n",
    "!bash -c \"cd pldm_envs/wall && mkdir -p presaved_datasets/rendered\"\n",
    "!bash -c \"cd pldm_envs/wall && PYTHONPATH=../.. python render_images.py --input_path presaved_datasets/ds_size_634_no_images.npz --output_path presaved_datasets/rendered/ds_size_634.npz --config configs/good_quality_data.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "R0c1K4y4LP0g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0c1K4y4LP0g",
    "outputId": "78a8e61f-842f-4bb8-b4a7-6bb4c6fa43a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Saved train trials to PLDM_hieral/wall_trials_train.npz\n",
      "Saved eval trials to PLDM_hieral/wall_trials_eval.npz\n"
     ]
    }
   ],
   "source": [
    "!python PLDM_hieral/generate_wall_trials.py \\\n",
    "  --config PLDM_hieral/configs/tworooms_l1.yaml \\\n",
    "  --output_train PLDM_hieral/wall_trials_train.npz \\\n",
    "  --output_eval PLDM_hieral/wall_trials_eval.npz \\\n",
    "  --n_eval 100 \\\n",
    "  --seed 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tMQkFnZsLP0g",
   "metadata": {
    "id": "tMQkFnZsLP0g"
   },
   "source": [
    "## Run L1 2M baseline (no L2)\n",
    "Original-size L1-only model (no parameter matching).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "LMzEqH1rLP0g",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMzEqH1rLP0g",
    "outputId": "23d7f81d-8b31-408c-9fd0-7a32786d9ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Saved train trials to PLDM_hieral/wall_trials_train.npz\n",
      "Saved eval trials to PLDM_hieral/wall_trials_eval.npz\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "estimating mean stds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 91.25it/s]\n",
      "starting training for 3 epochs\n",
      "resuming from WM-JEPA/tworooms_l1/epoch=150_sample_step=87552.ckpt\n",
      "/workspace/WM-JEPA/PLDM_hieral/pldm/train.py:358: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint)\n",
      "resumed from epoch 150 step 1358\n",
      "Epoch: 0it [00:00, ?it/s]\n",
      "No planning metrics found in summaries.\n",
      "Saved comparison CSV to /workspace/WM-JEPA/PLDM_hieral/WM-JEPA/tworooms_compare.csv\n"
     ]
    }
   ],
   "source": [
    "!python PLDM_hieral/generate_wall_trials.py \\\n",
    "  --config PLDM_hieral/configs/tworooms_l1.yaml \\\n",
    "  --output_train PLDM_hieral/wall_trials_train.npz \\\n",
    "  --output_eval PLDM_hieral/wall_trials_eval.npz \\\n",
    "  --n_eval 100 \\\n",
    "  --seed 42\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python PLDM_hieral/run_tworooms_compare.py \\\n",
    "  --config_l1 PLDM_hieral/configs/tworooms_l1.yaml \\\n",
    "  --mode l1 \\\n",
    "  --output_root {OUTPUT_ROOT} \\\n",
    "  --epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qWPrfI9LP0g",
   "metadata": {
    "id": "9qWPrfI9LP0g"
   },
   "source": [
    "## Run L1 (2M) vs L2 comparison\n",
    "Runs L1-only (`tworooms_l1.yaml`) then L2 (`tworooms_l2.yaml`) from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528cd3f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7528cd3f",
    "outputId": "3fe8c6ad-2157-4c92-956e-120d22eca002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Saved train trials to PLDM_hieral/wall_trials_train.npz\n",
      "Saved eval trials to PLDM_hieral/wall_trials_eval.npz\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "estimating mean stds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 91.73it/s]\n",
      "starting training for 3 epochs\n",
      "resuming from WM-JEPA/tworooms_l1/epoch=150_sample_step=87552.ckpt\n",
      "/workspace/WM-JEPA/PLDM_hieral/pldm/train.py:358: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint)\n",
      "resumed from epoch 150 step 1358\n",
      "Epoch: 0it [00:00, ?it/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "estimating mean stds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 91.11it/s]\n",
      "starting training for 3 epochs\n",
      "Epoch:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Batch:   0%|                                              | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:   0%| | 0/9 [00:00<?, ?i\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  11%| | 1/9 [00:00<00:05\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  22%|‚ñè| 2/9 [00:00<00:02\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  33%|‚ñé| 3/9 [00:01<00:01\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  44%|‚ñç| 4/9 [00:01<00:01\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  56%|‚ñå| 5/9 [00:01<00:00\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  67%|‚ñã| 6/9 [00:01<00:00\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  78%|‚ñä| 7/9 [00:01<00:00\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s:  89%|‚ñâ| 8/9 [00:02<00:00\u001b[A\n",
      "Loss: 7.4543, train: 0.689s, data: 0.010s, log: 0.000s: 100%|‚ñà| 9/9 [00:02<00:00\u001b[A\n",
      "Epoch:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1/4 [00:02<00:06,  2.28s/it]\n",
      "Batch:   0%|                                              | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:  11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 1/9 [00:00<00:01,  4.76it/s]\u001b[A\n",
      "Batch:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 2/9 [00:00<00:01,  4.76it/s]\u001b[A\n",
      "Batch:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 3/9 [00:00<00:01,  4.73it/s]\u001b[A\n",
      "Batch:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 4/9 [00:00<00:01,  4.71it/s]\u001b[A\n",
      "Batch:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 5/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 6/9 [00:01<00:00,  4.72it/s]\u001b[A\n",
      "Batch:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 7/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2/4 [00:04<00:04,  2.06s/it]\n",
      "Batch:   0%|                                              | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:  11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 1/9 [00:00<00:01,  4.76it/s]\u001b[A\n",
      "Batch:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 2/9 [00:00<00:01,  4.75it/s]\u001b[A\n",
      "Batch:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 3/9 [00:00<00:01,  4.74it/s]\u001b[A\n",
      "Batch:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 4/9 [00:00<00:01,  4.72it/s]\u001b[A\n",
      "Batch:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 5/9 [00:01<00:00,  4.72it/s]\u001b[A\n",
      "Batch:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 6/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 7/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Epoch:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3/4 [00:06<00:01,  1.99s/it]\n",
      "Batch:   0%|                                              | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batch:  11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 1/9 [00:00<00:01,  4.74it/s]\u001b[A\n",
      "Batch:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 2/9 [00:00<00:01,  4.75it/s]\u001b[A\n",
      "Batch:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 3/9 [00:00<00:01,  4.74it/s]\u001b[A\n",
      "Batch:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 4/9 [00:00<00:01,  4.75it/s]\u001b[A\n",
      "Batch:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 5/9 [00:01<00:00,  4.74it/s]\u001b[A\n",
      "Batch:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 6/9 [00:01<00:00,  4.74it/s]\u001b[A\n",
      "Batch:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 7/9 [00:01<00:00,  4.74it/s]\u001b[A\n",
      "Batch:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8/9 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "Batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  4.74it/s]\u001b[A\n",
      "evaluating epoch 3\n",
      "\n",
      "Probe l1 prediction epochs:   0%|                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:   5%|‚ñä               | 1/20 [00:22<07:02, 22.22s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  10%|‚ñà‚ñå              | 2/20 [00:44<06:37, 22.11s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  15%|‚ñà‚ñà‚ñç             | 3/20 [01:06<06:15, 22.07s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  20%|‚ñà‚ñà‚ñà‚ñè            | 4/20 [01:28<05:53, 22.10s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:17,  7.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  25%|‚ñà‚ñà‚ñà‚ñà            | 5/20 [01:50<05:31, 22.12s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:17,  7.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:12,  6.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/156 [00:15<00:06,  7.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 145/156 [00:20<00:01,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  30%|‚ñà‚ñà‚ñà‚ñà‚ñä           | 6/20 [02:12<05:09, 22.12s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 7/20 [02:34<04:47, 22.13s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 8/20 [02:56<04:24, 22.06s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  23%|‚ñà‚ñà‚ñà‚ñà‚ñç              | 36/156 [00:05<00:16,  7.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 72/156 [00:10<00:11,  7.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 108/156 [00:15<00:06,  7.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 144/156 [00:20<00:01,  7.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Probe l1 prediction epochs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 9/20 [03:18<04:02, 22.07s/it]\u001b[A\n",
      "\n",
      "Probe prediction step:   0%|                            | 0/156 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  22%|‚ñà‚ñà‚ñà‚ñà‚ñé              | 35/156 [00:05<00:17,  6.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Probe prediction step:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 71/156 [00:10<00:12,  7.02it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "!python PLDM_hieral/generate_wall_trials.py \\\n",
    "  --config PLDM_hieral/configs/tworooms_l1.yaml \\\n",
    "  --output_train PLDM_hieral/wall_trials_train.npz \\\n",
    "  --output_eval PLDM_hieral/wall_trials_eval.npz \\\n",
    "  --n_eval 100 \\\n",
    "  --seed 42\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python PLDM_hieral/run_tworooms_compare.py \\\n",
    "  --config_l1 PLDM_hieral/configs/tworooms_l1.yaml \\\n",
    "  --config_l2 PLDM_hieral/configs/tworooms_l2.yaml \\\n",
    "  --output_root {OUTPUT_ROOT} \\\n",
    "  --l2_from_scratch \\\n",
    "  --mode both \\\n",
    "  --epochs 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8a0b0",
   "metadata": {
    "id": "71f8a0b0"
   },
   "source": [
    "Outputs are saved to Google Drive under `/content/drive/MyDrive/PLDM_2mvs6m`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f9bc",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7e31f9bc",
    "outputId": "983392c3-ac6d-46c7-9782-3e91b09aecdd"
   },
   "outputs": [],
   "source": [
    "# Outputs are already in Drive.\n",
    "!ls -la {OUTPUT_ROOT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e7126-aed0-43be-a6d2-8661541559c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
